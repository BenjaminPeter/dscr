---
title: "dsc2BE_example.Rmd"
output: html_document
---

This document shows an example of the use of the function dsc2BE
which "converts" a dsc to a BatchExperiments registry so that you can
use the BatchExperiments pipeline to run the dsc on a cluster.

Note that as far as I can see the BatchExperiments pipeline does not
save everything that the dsc pipeline saves. Specifically the BE pipeline
only appears to save the final results of each method, which the way I have
set it up here is the score. In comparison dsc saves the data, output and score.

First I set up the DSC. There is one important change from the previous
dscr-template setup: if you look at datamaker.R the default to seed is NULL,
and the first line sets the
seed only if the seed is supplied. This is to help make it work with
the BE pipeline, which handles the seeds internally (which seems preferable to me now).


```{r}
library("dscr")
library("BatchExperiments")

dsc_eg=new.dsc("one-sample-location")
source("scenarios.R")
source("methods.R")
source("score.R")

reset_dsc(dsc_eg,force=TRUE)
res=run_dsc(dsc_eg)


```

Now convert to BE and test a job and then run all jobs. 
```{r}
system("rm -r one_sample_location-files")

reg=dsc2BE(dsc_eg,"one_sample_location")
summarizeExperiments(reg)

id1 <- findExperiments(reg, algo.pattern="mean")[1]
testJob(reg,id1)


chunked <- chunk(getJobIds(reg), n.chunks = 10, shuffle = TRUE)
timetaken=system.time(submitJobs(reg, chunked))

res2=reduceResultsExperiments(reg, ids=findDone(reg))

```

Now compare results
```{r}
aggregate(squared_error~algo+prob,data=res2,mean)
aggregate(squared_error~method+scenario,res,mean)

aggregate(abs_error~algo+prob,data=res2,mean)
aggregate(abs_error~method+scenario,res,mean)

```

